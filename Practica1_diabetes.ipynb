{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbOTJpwGXsqa"
      },
      "source": [
        "# 游닇 **Pr치ctica #1:** 游 Construcci칩n de una red neuronal para la predicci칩n de diabetes\n",
        "[![Open in Colab](hhttps://colab.research.google.com/assets/colab-badge.svg)](hhttps://colab.research.google.com/github/LoboaTeresa/Workshop-COF/blob/master/Practica1_diabetes.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM0Udz4rX1sd"
      },
      "source": [
        "### 1. Importar librer칤as necesarias\n",
        "\n",
        "Usaremos [Keras](https://keras.io/) para la definici칩n de nuestra red neuronal y [numpy](https://numpy.org/doc/stable/index.html) para varias operaciones matem치ticas y lectura de nustro dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "seed = 7 # para la reproducibilidad del experimento\n",
        "numpy.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REUUbURYSvi"
      },
      "source": [
        "### 2. Cargar el dataset: Pima Indians Diabetes Database\n",
        "\n",
        "Este dataset procede del Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EEUU. El objetivo de este dataset es diagnosticar si un paciente es diab칠tico o no, bas치ndose en 8 par치metros.\n",
        "\n",
        "* 1990\n",
        "* Dataset de clasificaci칩n binaria\n",
        "* 8 par치metros reales y enteros\n",
        "* 768 instancias\n",
        "* 100% de os pacientes son mujeres, de la etnia india Pima y > 21 a침os de edad.\n",
        "\n",
        "\n",
        "[link para descargar el dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset de los indios Pima.\n",
        "dataset = numpy.loadtxt(\"assets/pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# Dividir los datos en par치metros y label (o verdad).\n",
        "X = dataset[:, 0:8]\n",
        "Y = dataset[:, 8]\n",
        "\n",
        "# Dividir los datos en entrenamiento y test.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-160j4hYzvB"
      },
      "source": [
        "### 3. Definici칩n de la arquitectura:\n",
        "\n",
        "[Documentaci칩n de Keras](https://keras.io/api/layers/). 칄chale un ojo :)\n",
        "\n",
        "Vamos a construir una red neuronal de 4 capas densas o completamente conectadas.\n",
        "\n",
        "![Simple Network](assets/simple_network.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializamos el modelo de Keras\n",
        "model = Sequential()\n",
        "\n",
        "model.add(_INSERT_LAYER_HERE_(?, input_shape=?, activation='relu'))\n",
        "model.add(_INSERT_LAYER_HERE_(?, activation='relu'))\n",
        "model.add(_INSERT_LAYER_HERE_(?, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvfz1y-ZTEC"
      },
      "source": [
        "### 4. Compilar el modelo\n",
        "\n",
        "Una vez que el modelo esta definido, se procede a compilarlo. Al compilar el modelo, se llama a las librerias en el backend, en nuestro caso Tensorflow. En este caso el backend autom치ticamente selecciona la mejor forma de representar la red neuronal para entrenamiento y realizar predicciones en el hardware. Cuando se compila el modelo, se deben definir algunas propiedades adicionales requeridas para el entrenamiento del modelo:\n",
        "\n",
        "* **Funci칩n de p칠rdida** o loss function, que es utilizada para evaluar los pesos.\n",
        "* **Optimizador** utilizado para buscar entre los pesos de la red y algunas m칠tricas opcionales que se require colectar y reportar durante el entrenamiento.\n",
        "\n",
        "En este ejemplo utilizaremos una funci칩n de perdida **binary cross entropy**, que es una funci칩n logar칤tmica de perdida. Se utilizar치 una funci칩n para los calcular los gradientes llamada **adam**.\n",
        "\n",
        "Al ser un problema de clasificaci칩n binaria, se evaluar치 la precision de la clasificaci칩n utilizando accuracy (n칰mero de predicciones correctas/n칰mero de datos totales).\n",
        "\n",
        "_M치s info sobre binary cross entropy [aqu칤](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)._\n",
        "\n",
        "_M치s info sobre el optimizador Adam [aqu칤](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuD9Pc0_ZjR7"
      },
      "source": [
        "### 5. Entrenar el modelo\n",
        "\n",
        "Una vez que este definido y compilado tu modelo, es el momento de ejecutar el modelo con los datos.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta cierto numero de veces utilizando el dataset, este numero de veces es llamado epochs, y se define utilizando el par치metro _epochs_. Tambi칠n podemos definir el numero de instancias que son evaluadas antes de que los pesos sean actualizados en la red neuronal. Este par치metro se llama _batch size_.\n",
        "\n",
        "En este problema se definir치 un numero peque침o de epochs (150) y un valor relativamente peque침o de el batch (10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, Y_train, epochs=150, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrALvMlaWBT"
      },
      "source": [
        "### 6. Eval칰a tu modelo\n",
        "\n",
        "Ahora que hemos entrenado nuestro modelo utilizando el dataset completo, debemos evaluar el rendimiento de la red neuronal. Estos nos data una idea que tan bien modelamos nuestro dataset, pero no tendremos idea como se desempe침ara el modelo con datos nuevos.\n",
        "\n",
        "En la realidad se debe de separar el dataset de training y el dataset de testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqtLHFfSkJEm",
        "outputId": "a1b8ddbb-2f90-4ab4-d2d5-f3c04f810c6c"
      },
      "outputs": [],
      "source": [
        "_, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjkCqodNajko"
      },
      "source": [
        "### 7. Realiza una predicci칩n (inferencia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3HFAW3Nam6W",
        "outputId": "b40fb2db-1444-4ef2-8e3e-3624181e1648"
      },
      "outputs": [],
      "source": [
        "# make class predictions with the model\n",
        "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# summarize the first 5 cases\n",
        "for i in range(5):\n",
        " print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], Y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgdbL3UQaeQy"
      },
      "source": [
        "**춰BUEN TRABAJO!** Ahora que hemos entrenado nuestro modelo, podemos utilizarlo para realizar predicciones sobre datos nuevos.\n",
        "\n",
        "### 8. Conclusi칩n\n",
        "\n",
        "Hemos seguido los siguientes pasos:\n",
        "\n",
        "* Cargar datos.\n",
        "* Definir el modelo de la red neuronal en Keras.\n",
        "* Compilar el modelo.\n",
        "* Entrenar el modelo.\n",
        "* Evaluar el modelo con datos"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

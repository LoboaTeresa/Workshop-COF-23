{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbOTJpwGXsqa"
      },
      "source": [
        "# 📝 **Práctica #1:** 🚀 Construcción de una red neuronal para la predicción de diabetes\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LoboaTeresa/Workshop-COF-23/blob/main/Practica1_diabetes.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REUUbURYSvi"
      },
      "source": [
        "### 1. Cargar el dataset: Pima Indians Diabetes Database\n",
        "\n",
        "Este dataset procede del Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EEUU. El objetivo de este dataset es diagnosticar si un paciente es diabético o no, basándose en 8 parámetros.\n",
        "\n",
        "* 1990\n",
        "* Dataset de clasificación binaria\n",
        "* 8 parámetros reales y enteros\n",
        "* 768 instancias\n",
        "* 100% de os pacientes son mujeres, de la etnia india Pima y > 21 años de edad.\n",
        "\n",
        "Parámetros:\n",
        "1. Embarazos\n",
        "2. Glocosa\n",
        "3. Presión Arterial\n",
        "4. Espesor de la piel\t\n",
        "5. Insulina\n",
        "6. IMC\n",
        "7. DiabetesPedigreeFunction\n",
        "8. Edad\n",
        "9. Diagnósitco (1: diabético, 0: no diabético)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# descarga el dataset:\n",
        "!git clone https://github.com/LoboaTeresa/Workshop-COF-23.git\n",
        "!cp /content/Workshop-COF-23/assets/pima-indians-diabetes.csv /content/pima-indians-diabetes.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seed = 7 # para la reproducibilidad del experimento\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset de los indios Pima.\n",
        "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# Dividir los datos en parámetros y label (o verdad).\n",
        "X = dataset[:, 0:8]\n",
        "Y = dataset[:, 8]\n",
        "\n",
        "# Dividir los datos en entrenamiento y test.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-160j4hYzvB"
      },
      "source": [
        "### 2. Definición de la arquitectura:\n",
        "\n",
        "[Documentación de Keras](https://keras.io/api/layers/). Échale un ojo :)\n",
        "\n",
        "Vamos a construir una red neuronal de 4 capas densas o completamente conectadas.\n",
        "\n",
        "![Simple Network](https://github.com/LoboaTeresa/Workshop-COF-23/blob/main/assets/simple_net.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializamos el modelo de Keras\n",
        "model = Sequential()\n",
        "\n",
        "model.add(_INSERT_LAYER_HERE_(?, input_shape=?, activation='relu'))\n",
        "model.add(_INSERT_LAYER_HERE_(?, activation='relu'))\n",
        "model.add(_INSERT_LAYER_HERE_(?, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvfz1y-ZTEC"
      },
      "source": [
        "### 3. Compilar el modelo\n",
        "\n",
        "Una vez que el modelo esta definido, se procede a compilarlo. Al compilar el modelo, se llama a las librerias en el backend, en nuestro caso Tensorflow. En este caso el backend automáticamente selecciona la mejor forma de representar la red neuronal para entrenamiento y realizar predicciones en el hardware. Cuando se compila el modelo, se deben definir algunas propiedades adicionales requeridas para el entrenamiento del modelo:\n",
        "\n",
        "* **Función de pérdida** o loss function, que es utilizada para evaluar los pesos.\n",
        "* **Optimizador** utilizado para buscar entre los pesos de la red y algunas métricas opcionales que se require colectar y reportar durante el entrenamiento.\n",
        "\n",
        "En este ejemplo utilizaremos una función de perdida **binary cross entropy**, que es una función logarítmica de perdida. Se utilizará una función para los calcular los gradientes llamada **adam**.\n",
        "\n",
        "Al ser un problema de clasificación binaria, se evaluará la precision de la clasificación utilizando accuracy (número de predicciones correctas/número de datos totales).\n",
        "\n",
        "_Más info sobre binary cross entropy [aquí](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)._\n",
        "\n",
        "_Más info sobre el optimizador Adam [aquí](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuD9Pc0_ZjR7"
      },
      "source": [
        "### 4. Entrenar el modelo\n",
        "\n",
        "Una vez que este definido y compilado tu modelo, es el momento de ejecutar el modelo con los datos.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta cierto numero de veces utilizando el dataset, este numero de veces es llamado epochs, y se define utilizando el parámetro _epochs_. También podemos definir el numero de instancias que son evaluadas antes de que los pesos sean actualizados en la red neuronal. Este parámetro se llama _batch size_.\n",
        "\n",
        "En este problema se definirá un numero pequeño de epochs (150) y un valor relativamente pequeño de el batch (10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, Y_train, epochs=150, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrALvMlaWBT"
      },
      "source": [
        "### 5. Evalúa tu modelo\n",
        "\n",
        "Ahora que hemos entrenado nuestro modelo utilizando el dataset completo, debemos evaluar el rendimiento de la red neuronal. Estos nos data una idea que tan bien modelamos nuestro dataset, pero no tendremos idea como se desempeñara el modelo con datos nuevos.\n",
        "\n",
        "En la realidad se debe de separar el dataset de training y el dataset de testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqtLHFfSkJEm",
        "outputId": "a1b8ddbb-2f90-4ab4-d2d5-f3c04f810c6c"
      },
      "outputs": [],
      "source": [
        "_, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Matriz de confusión:** es una herramienta que permite visualizar el desempeño de un algoritmo  de aprendizaje supervisado. Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real., o sea en términos prácticos nos permite ver  qué tipos de aciertos y errores está teniendo nuestro modelo a la hora de pasar por el proceso de aprendizaje con los datos.\n",
        "\n",
        "<img src=\"https://github.com/LoboaTeresa/Workshop-COF-23/blob/main/assets/CM_explained.png?raw=true\"\n",
        "     alt=\"confusionMatrix\"\n",
        "     width=400 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
        "cm = metrics.confusion_matrix(Y_test, predictions)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1])\n",
        "cm_display.plot()\n",
        "plt.figure(figsize=(50, 50), dpi=80)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjkCqodNajko"
      },
      "source": [
        "### 6. Realiza una predicción (inferencia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3HFAW3Nam6W",
        "outputId": "b40fb2db-1444-4ef2-8e3e-3624181e1648"
      },
      "outputs": [],
      "source": [
        "# summarize the first 5 cases\n",
        "for i in range(5):\n",
        " print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], Y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgdbL3UQaeQy"
      },
      "source": [
        "**¡BUEN TRABAJO!** Ahora que hemos entrenado nuestro modelo, podemos utilizarlo para realizar predicciones sobre datos nuevos.\n",
        "\n",
        "### 7. Conclusión\n",
        "\n",
        "Hemos seguido los siguientes pasos:\n",
        "\n",
        "* Cargar datos.\n",
        "* Definir el modelo de la red neuronal en Keras.\n",
        "* Compilar el modelo.\n",
        "* Entrenar el modelo.\n",
        "* Evaluar el modelo con datos"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

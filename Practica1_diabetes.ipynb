{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbOTJpwGXsqa"
      },
      "source": [
        "# 📝 **Práctica #1:** 🚀 Construcción de una red neuronal para la predicción de diabetes\n",
        "[![Open in Colab](hhttps://colab.research.google.com/assets/colab-badge.svg)](hhttps://colab.research.google.com/github/LoboaTeresa/Workshop-COF/blob/master/Practica1_diabetes.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM0Udz4rX1sd"
      },
      "source": [
        "### 1. Importar librerías necesarias\n",
        "\n",
        "Usaremos [Keras](https://keras.io/) para la definición de nuestra red neuronal y [numpy](https://numpy.org/doc/stable/index.html) para varias operaciones matemáticas y lectura de nustro dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "seed = 7 # para la reproducibilidad del experimento\n",
        "numpy.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REUUbURYSvi"
      },
      "source": [
        "### 2. Cargar el dataset: Pima Indians Diabetes Database\n",
        "\n",
        "Este dataset procede del Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EEUU. El objetivo de este dataset es diagnosticar si un paciente es diabético o no, basándose en 8 parámetros.\n",
        "\n",
        "* 1990\n",
        "* Dataset de clasificación binaria\n",
        "* 8 parámetros reales y enteros\n",
        "* 768 instancias\n",
        "* 100% de os pacientes son mujeres, de la etnia india Pima y > 21 años de edad.\n",
        "\n",
        "\n",
        "[link para descargar el dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset de los indios Pima.\n",
        "dataset = numpy.loadtxt(\"assets/pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# Dividir los datos en parámetros y label (o verdad).\n",
        "X = dataset[:, 0:8]\n",
        "Y = dataset[:, 8]\n",
        "\n",
        "# Dividir los datos en entrenamiento y test.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-160j4hYzvB"
      },
      "source": [
        "### 3. Definición de la arquitectura:\n",
        "\n",
        "[Documentación de Keras](https://keras.io/api/layers/). Échale un ojo :)\n",
        "\n",
        "Vamos a construir una red neuronal de 4 capas densas o completamente conectadas.\n",
        "\n",
        "![Simple Network](assets/simple_network.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializamos el modelo de Keras\n",
        "model = Sequential()\n",
        "\n",
        "model.add(_INSERT_LAYER_HERE_(?, input_shape=?, activation='relu'))\n",
        "model.add(_INSERT_LAYER_HERE_(?, activation='relu'))\n",
        "model.add(_INSERT_LAYER_HERE_(?, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvfz1y-ZTEC"
      },
      "source": [
        "### 4. Compilar el modelo\n",
        "\n",
        "Una vez que el modelo esta definido, se procede a compilarlo. Al compilar el modelo, se llama a las librerias en el backend, en nuestro caso Tensorflow. En este caso el backend automáticamente selecciona la mejor forma de representar la red neuronal para entrenamiento y realizar predicciones en el hardware. Cuando se compila el modelo, se deben definir algunas propiedades adicionales requeridas para el entrenamiento del modelo:\n",
        "\n",
        "* **Función de pérdida** o loss function, que es utilizada para evaluar los pesos.\n",
        "* **Optimizador** utilizado para buscar entre los pesos de la red y algunas métricas opcionales que se require colectar y reportar durante el entrenamiento.\n",
        "\n",
        "En este ejemplo utilizaremos una función de perdida **binary cross entropy**, que es una función logarítmica de perdida. Se utilizará una función para los calcular los gradientes llamada **adam**.\n",
        "\n",
        "Al ser un problema de clasificación binaria, se evaluará la precision de la clasificación utilizando accuracy (número de predicciones correctas/número de datos totales).\n",
        "\n",
        "_Más info sobre binary cross entropy [aquí](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)._\n",
        "\n",
        "_Más info sobre el optimizador Adam [aquí](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuD9Pc0_ZjR7"
      },
      "source": [
        "### 5. Entrenar el modelo\n",
        "\n",
        "Una vez que este definido y compilado tu modelo, es el momento de ejecutar el modelo con los datos.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta cierto numero de veces utilizando el dataset, este numero de veces es llamado epochs, y se define utilizando el parámetro _epochs_. También podemos definir el numero de instancias que son evaluadas antes de que los pesos sean actualizados en la red neuronal. Este parámetro se llama _batch size_.\n",
        "\n",
        "En este problema se definirá un numero pequeño de epochs (150) y un valor relativamente pequeño de el batch (10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, Y_train, epochs=150, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrALvMlaWBT"
      },
      "source": [
        "### 6. Evalúa tu modelo\n",
        "\n",
        "Ahora que hemos entrenado nuestro modelo utilizando el dataset completo, debemos evaluar el rendimiento de la red neuronal. Estos nos data una idea que tan bien modelamos nuestro dataset, pero no tendremos idea como se desempeñara el modelo con datos nuevos.\n",
        "\n",
        "En la realidad se debe de separar el dataset de training y el dataset de testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqtLHFfSkJEm",
        "outputId": "a1b8ddbb-2f90-4ab4-d2d5-f3c04f810c6c"
      },
      "outputs": [],
      "source": [
        "_, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjkCqodNajko"
      },
      "source": [
        "### 7. Realiza una predicción (inferencia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3HFAW3Nam6W",
        "outputId": "b40fb2db-1444-4ef2-8e3e-3624181e1648"
      },
      "outputs": [],
      "source": [
        "# make class predictions with the model\n",
        "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# summarize the first 5 cases\n",
        "for i in range(5):\n",
        " print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], Y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgdbL3UQaeQy"
      },
      "source": [
        "**¡BUEN TRABAJO!** Ahora que hemos entrenado nuestro modelo, podemos utilizarlo para realizar predicciones sobre datos nuevos.\n",
        "\n",
        "### 8. Conclusión\n",
        "\n",
        "Hemos seguido los siguientes pasos:\n",
        "\n",
        "* Cargar datos.\n",
        "* Definir el modelo de la red neuronal en Keras.\n",
        "* Compilar el modelo.\n",
        "* Entrenar el modelo.\n",
        "* Evaluar el modelo con datos"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
